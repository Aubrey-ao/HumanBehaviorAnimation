Human Behavior Animation
============

This project explores the human behavior animation, including gesture, etc., as part of my graduate research at Peking University, supervised by <a href="http://libliu.info/">Libin Liu</a>.

------------
**SIGGRAPH Asia 2022** ([Technical Best Paper Award](https://sa2022.siggraph.org/en/attend/award-winners/))<br />
**Rhythmic Gesticulator: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings**<br />
<sub>
<a href="https://aubrey-ao.github.io/">Tenglong Ao</a>,
<a href="https://talegqz.github.io/">Qingzhe Gao</a>,
<a href="https://thorin666.github.io/">Yuke Lou</a>,
<a href="http://baoquanchen.info/">Baoquan Chen</a>,
<a href="http://libliu.info/">Libin Liu</a>,
ACM Trans. Graph. 41, 6, Article 209.
<sub>
------------
<img src="Media/SIGGRAPH_Asia_2022/teaser.png" width="100%">

<p align="justify">
Automatic synthesis of realistic co-speech gestures is an increasingly important yet challenging task in artificial embodied agent creation. In this work, we present a novel co-speech gesture synthesis method that achieves convincing results both on the rhythm and semantics. For the rhythm, our system contains a robust rhythm-based segmentation pipeline to ensure the temporal coherence between the vocalization and gestures explicitly. For the gesture semantics, we devise a mechanism to effectively disentangle both low- and high-level neural embeddings of speech and motion based on linguistic theory. The high-level embedding corresponds to semantics, while the low-level embedding relates to subtle variations. Lastly, we build correspondence between the hierarchical embeddings of the speech and the motion, resulting in rhythm and semantics-aware gesture synthesis.
</p>

<p align="center">
-
Video (<a href="https://www.youtube.com/watch?v=qy2MrNhsoIs/">YouTube</a> | <a href="https://www.bilibili.com/video/BV1G24y1d7Tt/">Bilibili</a>)
-
Paper (<a href="https://arxiv.org/abs/2210.01448/">arXiv</a>)
-
Code (<a href="https://github.com/Aubrey-ao/HumanBehaviorAnimation/tree/main/HumanBehaviorAnimation/RhythmicGesticulator/Simplified_Version">github</a>)
-
Dataset (<a href="https://github.com/Aubrey-ao/HumanBehaviorAnimation/tree/main/HumanBehaviorAnimation/RhythmicGesticulator/MOCCA_Gesture_Dataset/">github</a>)
-
<br>
-
Explained (<a href="https://www.youtube.com/watch?v=DO_W8plFWco/">YouTube(English)</a> | <a href="https://zhuanlan.zhihu.com/p/573998492/">知乎(Chinese)</a>)
-
</p>

<p align="center">
<a href="https://www.youtube.com/watch?v=qy2MrNhsoIs/">
<img width="60%" src="Media/SIGGRAPH_Asia_2022/thumbnail_YouTube.png">
</a>
</p>

Acknowledgement
============
The layout of this project is highly inspired by <a href="https://github.com/sebastianstarke/AI4Animation">AI4Animation</a> repo.

Copyright Information
============
This project is only for research or education purposes, and not freely available for commercial use or redistribution.
